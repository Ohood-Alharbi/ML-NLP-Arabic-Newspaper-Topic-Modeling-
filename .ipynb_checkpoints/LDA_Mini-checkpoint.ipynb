{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import pyodbc\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlalchemy\n",
    "import nltk\n",
    "import timeit\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "import unicodedata\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = 't5.database.windows.net'\n",
    "#server='192.168.0.100'\n",
    "database = 'T5' \n",
    "username = 'T5' \n",
    "password = 'My404Data' \n",
    "\n",
    "cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "cursor = cnxn.cursor()\n",
    "df= pd.read_sql(\"select top(1000) * from texts\",cnxn)\n",
    "#df.to_csv(\"texts.csv\",encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= pd.read_sql(\"select  * from Articles\",cnxn)\n",
    "df2.to_csv(\"articles.csv\",encoding='utf-8-sig')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"texts.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.merge(df2, df, how='inner', left_on='A_ID', right_on='T_AID')\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Words_Number'] = df3['T_Text'].astype(str).apply(lambda x : len(x.split()))\n",
    "df3['T_Text'] = df3['T_Text'].apply(lambda x:unicodedata.normalize(\"NFKD\", str(x)).replace('\\n', ''))\n",
    "# Wrong Texts (i.e. very long/short texts)\n",
    "df3 = df3[((df3['Words_Number'] > 150) & (df3['Words_Number'] < 500))]\n",
    "df3 = df3.reset_index()\n",
    "#df3\n",
    "Articles = list(df3['T_Text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordtoken = word_tokenize(' '.join(Articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wordtoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words = stopwords.words('arabic'),token_pattern='\\\\b[\\u0621-\\u064A][\\u0621-\\u064A]+\\\\b')\n",
    "#count_vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words = stopwords.words('arabic'),token_pattern='\\\\b[\\u0621-\\u064A][\\u0621-\\u064A]+\\\\b')\n",
    " \n",
    "X = count_vectorizer.fit_transform(Articles)\n",
    "cols = count_vectorizer.get_feature_names_out()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(cols[100064])\n",
    "#X.toarray()\n",
    "# without stop words removal we got 138,561\n",
    "# using arabic stop words we got 138,108\n",
    "#print(X)\n",
    "#print(X[1:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = dict((v, k) for k, v in count_vectorizer.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_word = count_vectorizer.transform(Articles).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ab = pd.DataFrame(doc_word.toarray(), count_vectorizer.get_feature_names())\n",
    "ab['Total'] = ab.sum(axis=1)\n",
    "ab = ab.sort_values('Total',ascending=False)\n",
    "ab.head(60)\n",
    "#ab2\n",
    "#ab2.shape\n",
    "#ab2.max()\n",
    "#ab2.argmax()\n",
    "#ab[ab2.argmax():ab2.argmax()+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = matutils.Sparse2Corpus(doc_word)\n",
    "#models.LdaModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus=corpus, num_topics=10, id2word=id2word, passes=3,iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = matutils.Sparse2Corpus(doc_word)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda.print_topics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the docs from the word space to the topic space (like \"transform\" in sklearn)\n",
    "lda_corpus = lda[corpus]\n",
    "#del lda_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['T_Text'].str.contains(\"ية \")].iloc[4].T_Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the documents' topic vectors in a list so we can take a peak\n",
    "lda_docs = [doc for doc in lda_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = pd.DataFrame(columns=['ArticleID','TopicID','Perc'])\n",
    "def func(x):\n",
    "    #pass\n",
    "    #print(x['index'])\n",
    "    global df6\n",
    "    for x1 in x['D']:\n",
    "        #print(x1[1])\n",
    "        #df6.iloc[-1] = [0,x1[0],x1[1]]\n",
    "        df6 = df6.append(pd.DataFrame(columns=['ArticleID','TopicID','Perc'],data=[[x['index'],x1[0],x1[1]]]))\n",
    "        #df6.index = df6.index+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the document vectors in the topic space for the first 5 documents\n",
    "lda_docs[0:50]\n",
    "df5 = pd.DataFrame(columns = [\"D\"],data=zip(lda_docs))\n",
    "#a = np.array(lda_docs[2])\n",
    "#a[:,1].max()\n",
    "\n",
    "#df5['Perc'] = df5['D'].apply(lambda x:list(zip(x))[0][0][1])\n",
    "df5 = df5.reset_index()\n",
    "df5.apply(lambda x:func(x),axis=1)\n",
    "df5.head()\n",
    "df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df3 = df3.reset_index()\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df6[df6['TopicID']==12]\n",
    "df7 = pd.merge(df6,df3,how='left',left_on='ArticleID',right_on='index')\n",
    "df7[df7['TopicID']==9][['ArticleID','TopicID','Perc','A_Cat','A_Title1','A_Title2','T_Text']]\n",
    "\n",
    "#df7[df7['Index']==4][['ArticleID','TopicID','Perc','A_Cat','A_Title1','A_Title2','T_Text']]\n",
    "Articles[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Articles[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=lda.show_topics(num_topics=12, num_words=5,formatted=False)\n",
    "topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "\n",
    "#Below Code Prints Topics and Words\n",
    "for topic,words in topics_words:\n",
    "    print(str(topic)+ \"::\"+ str(words))\n",
    "print()\n",
    "\n",
    "#Below Code Prints Only Words \n",
    "for topic,words in topics_words:\n",
    "    print(\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
