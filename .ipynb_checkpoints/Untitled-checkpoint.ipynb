{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import pyodbc\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    " \n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    " \n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis  # don't skip this\n",
    "pyLDAvis.enable_notebook()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    " \n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('arabic')\n",
    "#stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = 't5.database.windows.net'\n",
    "server='DESKTOP-ASD2RK7\\SQLEXPRESS'\n",
    "database = 'T5' \n",
    "username = 'T5' \n",
    "password = 'My404Data' \n",
    "database = 'Riyadh2'\n",
    "username ='sab'\n",
    "password = 'tata2015'\n",
    "cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "cursor = cnxn.cursor()\n",
    "df= pd.read_sql(\"select   * from texts inner join articles on t_aid = a_id where a_words>150 and a_words <501 \",cnxn)\n",
    "df.to_csv(\"texts3.csv\",encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Words_Number'] = df3['T_Text'].astype(str).apply(lambda x : len(x.split()))\n",
    "df3['T_Text'] = df3['T_Text'].apply(lambda x:unicodedata.normalize(\"NFKD\", str(x)).replace('\\n', ''))\n",
    "# Wrong Texts (i.e. very long/short texts)\n",
    "df3 = df3[((df3['Words_Number'] > 150) & (df3['Words_Number'] < 500))]\n",
    "df3 = df3.reset_index()\n",
    "df3 = df3.drop_duplicates(subset =\"T_Text\")\n",
    "#df3\n",
    "data = list(df3['T_Text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['قال', 'صاحب', 'السمو', 'الملكي', 'الامير', 'فيصل', 'بن', 'مشعل', 'بن', 'سعود', 'بن', 'عبدالعزيز', 'امير', 'منطقة', 'القصيم', 'ان', 'ما', 'امر', 'به', 'سيدي', 'خادم', 'الحرمين', 'الشريفين', 'الملك', 'سلمان', 'بن', 'عبدالعزيز', 'حفظه', 'الله', 'من', 'تحمل', 'الدولة', 'من', 'رواتب', 'العاملين', 'السعوديين', 'في', 'القطاع', 'الخاص', 'الذين', 'تاثروا', 'من', 'التداعيات', 'الحالية', 'جراء', 'انتشار', 'فيروس', 'كورونا', 'من', 'خلال', 'نظام', 'ساند', 'ياتي', 'في', 'اطار', 'حرصه', 'حفظه', 'الله', 'على', 'ابنايه', 'وبناته', 'المواطنين', 'سواء', 'العاملين', 'او', 'كذلك', 'اصحاب', 'العمل', 'وقال', 'امير', 'القصيم', 'ان', 'التدابير', 'الوقايية', 'التي', 'اتخذتها', 'المملكة', 'للحد', 'من', 'انتشار', 'هذا', 'الوباء', 'اثرت', 'لا', 'شك', 'على', 'قطاع', 'الاعمال', 'فكانت', 'اليد', 'الحانية', 'من', 'لدن', 'مقام', 'خادم', 'الحرمين', 'الشريفين', 'الذي', 'يسهر', 'من', 'اجل', 'رفاهية', 'شعبه', 'مهما', 'كلف', 'ذلك', 'وقال', 'الامير', 'فيصل', 'بن', 'مشعل', 'ان', 'امر', 'الملك', 'المفدى', 'بتحمل', 'الدولة', 'لما', 'يصل', 'لتسعة', 'مليارات', 'ريال', 'ياتي', 'في', 'سبيل', 'استقرار', 'هذا', 'الوطن', 'والمواطن', 'وهذا', 'ليس', 'بالامر', 'الجديد', 'علينا', 'في', 'هذه', 'البلاد', 'فعناية', 'سيدي', 'خادم', 'الحرمين', 'الشريفين', 'وسمو', 'ولي', 'عهده', 'الامين', 'بشوون', 'المواطنين', 'وكذلك', 'رجال', 'الاعمال', 'ليس', 'بالامر', 'المستجد', 'بل', 'هي', 'اصل', 'اصيل', 'في', 'كل', 'مناسبة', 'وعند', 'اي', 'حدث', 'واختتم', 'امير', 'القصيم', 'تصريحه', 'سايلا', 'المولى', 'سبحانه', 'وتعالى', 'بان', 'يحفظ', 'خادم', 'الحرمين', 'الشريفين', 'وسمو', 'ولي', 'عهده', 'الامين', 'لهذه', 'البلاد', 'وان', 'يرفع', 'هذه', 'الجايحة', 'عن', 'بلادنا', 'وعن', 'العالم', 'اجمع']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    " \n",
    "data_words = list(sent_to_words(data))\n",
    " \n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stop words\n",
    "tokenization\n",
    "id2word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 01:27:22,022 : INFO : collecting all words and their counts\n",
      "2021-11-11 01:27:22,023 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2021-11-11 01:27:25,636 : INFO : PROGRESS: at sentence #10000, processed 2652644 words and 1300462 word types\n",
      "2021-11-11 01:27:29,327 : INFO : PROGRESS: at sentence #20000, processed 5320229 words and 2274247 word types\n",
      "2021-11-11 01:27:33,051 : INFO : PROGRESS: at sentence #30000, processed 7983989 words and 3127793 word types\n",
      "2021-11-11 01:27:36,725 : INFO : PROGRESS: at sentence #40000, processed 10639821 words and 3882515 word types\n",
      "2021-11-11 01:27:40,449 : INFO : PROGRESS: at sentence #50000, processed 13286907 words and 4577211 word types\n",
      "2021-11-11 01:27:44,220 : INFO : PROGRESS: at sentence #60000, processed 15926177 words and 5270860 word types\n",
      "2021-11-11 01:27:48,599 : INFO : PROGRESS: at sentence #70000, processed 18610030 words and 5924368 word types\n",
      "2021-11-11 01:27:52,510 : INFO : PROGRESS: at sentence #80000, processed 21245712 words and 6514106 word types\n",
      "2021-11-11 01:27:56,277 : INFO : PROGRESS: at sentence #90000, processed 23847524 words and 7096870 word types\n",
      "2021-11-11 01:27:59,952 : INFO : PROGRESS: at sentence #100000, processed 26452683 words and 7701370 word types\n",
      "2021-11-11 01:28:03,682 : INFO : PROGRESS: at sentence #110000, processed 29055780 words and 8308968 word types\n",
      "2021-11-11 01:28:07,597 : INFO : PROGRESS: at sentence #120000, processed 31643461 words and 8916010 word types\n",
      "2021-11-11 01:28:11,453 : INFO : PROGRESS: at sentence #130000, processed 34230939 words and 9532295 word types\n",
      "2021-11-11 01:28:14,410 : INFO : collected 9980959 token types (unigram + bigrams) from a corpus of 36162652 words and 137249 sentences\n",
      "2021-11-11 01:28:14,412 : INFO : merged Phrases<9980959 vocab, min_count=5, threshold=100, max_vocab_size=40000000>\n",
      "2021-11-11 01:28:14,414 : INFO : Phrases lifecycle event {'msg': 'built Phrases<9980959 vocab, min_count=5, threshold=100, max_vocab_size=40000000> in 52.39s', 'datetime': '2021-11-11T01:28:14.413847', 'gensim': '4.1.2', 'python': '3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n",
      "2021-11-11 01:28:14,415 : INFO : collecting all words and their counts\n",
      "2021-11-11 01:28:14,418 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2021-11-11 01:28:22,972 : INFO : PROGRESS: at sentence #10000, processed 2390493 words and 1376781 word types\n",
      "2021-11-11 01:28:31,464 : INFO : PROGRESS: at sentence #20000, processed 4785549 words and 2438221 word types\n",
      "2021-11-11 01:28:40,219 : INFO : PROGRESS: at sentence #30000, processed 7172991 words and 3377753 word types\n",
      "2021-11-11 01:28:49,137 : INFO : PROGRESS: at sentence #40000, processed 9551425 words and 4215371 word types\n",
      "2021-11-11 01:28:57,863 : INFO : PROGRESS: at sentence #50000, processed 11916540 words and 4991706 word types\n",
      "2021-11-11 01:29:06,974 : INFO : PROGRESS: at sentence #60000, processed 14276117 words and 5767548 word types\n",
      "2021-11-11 01:29:16,157 : INFO : PROGRESS: at sentence #70000, processed 16681565 words and 6504867 word types\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    " \n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    " \n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    " \n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    " \n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    " \n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words_bigrams)\n",
    " \n",
    "# Create Corpus\n",
    "texts = data_words_bigrams\n",
    " \n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    " \n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
